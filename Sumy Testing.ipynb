{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T15:53:26.481146Z",
     "start_time": "2021-02-12T15:53:24.451817Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from yaml import safe_load\n",
    "import pandas as pd\n",
    "import re\n",
    "# Extractive\n",
    "from nltk.tokenize import WordPunctTokenizer, sent_tokenize, PunktSentenceTokenizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "from sumy.summarizers.sum_basic import SumBasicSummarizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.summarizers.random import RandomSummarizer\n",
    "from sumy.summarizers.kl import KLSummarizer\n",
    "from sumy.summarizers.reduction import ReductionSummarizer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "from sumy.evaluation.rouge import (rouge_1, rouge_2, \n",
    "                                   rouge_l_sentence_level,\n",
    "                                   rouge_l_summary_level, rouge_n)\n",
    "from sumy.models.dom._sentence import Sentence\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T14:09:44.918894Z",
     "start_time": "2021-02-12T14:09:44.390231Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\t SUMMARIZATION REVIEW\t\t\n",
      "\n",
      "[INFO] Loading configuration\n",
      "[INFO] Loading json data from\n",
      "[INFO] Removing articles without summary or paragraphs\n",
      "[INFO] Size before cleaning: 10335\n",
      "[INFO] Size after cleaning: 9480\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\t\\t SUMMARIZATION REVIEW\\t\\t\\n\")\n",
    "print('[INFO] Loading configuration')\n",
    "with open(\"./config.yml\", 'r') as file:\n",
    "    config_var = safe_load(file)[\"main\"]\n",
    "\n",
    "print(\"[INFO] Loading json data from\")\n",
    "with open(\n",
    "        str(config_var['dataset_folder'])+\"/\"+str(config_var['data_to_use']),\n",
    "        'r'\n",
    "        ) as file:\n",
    "    data = pd.DataFrame(json.load(file))\n",
    "    \n",
    "print(\"[INFO] Removing articles without summary or paragraphs\")\n",
    "print(\"[INFO] Size before cleaning:\", len(data))\n",
    "data = data[(data[\"Summary\"].map(len) >= 1) &\n",
    "            (data[\"Paragraphs\"].map(len) >= 1)]\n",
    "print(\"[INFO] Size after cleaning:\", len(data))    \n",
    "data[\"Paragraphs_as_string\"] = data[\"Paragraphs\"].apply(\n",
    "    lambda x: \"\\n\\n\".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T10:05:35.636080Z",
     "start_time": "2021-02-12T10:05:35.629984Z"
    }
   },
   "outputs": [],
   "source": [
    "data = data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T10:11:33.361696Z",
     "start_time": "2021-02-12T10:10:33.273825Z"
    }
   },
   "outputs": [],
   "source": [
    "punkt_tokenizer = PunktSentenceTokenizer(\n",
    "    train_text=\"\\n\".join([sent for sent in data[\"Paragraphs_as_string\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T10:12:11.578420Z",
     "start_time": "2021-02-12T10:12:11.570073Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Raking in cash for Biden, the former president eases off the gloves: This is your morning tip sheet.']\n",
      "['In the early fall, Ms. Warren seemed to be the Democratic candidate to beat in Iowa.', 'But many voters have had second thoughts over how her sweeping agenda would sell against President Trump.']\n",
      "['Senator David Perdue, a Republican, drew a quick rebuke from his Democratic opponent, Jon Ossoff, who said the Facebook ad employed the “least original anti-Semitic trope in history.”']\n",
      "['But will Republicans’ celebration of their president deliver the agenda-setting boost he needs?']\n",
      "['“Democrats were organizing with every tool that didn’t require crossing the six-foot social distancing barrier,” the chair of the state Democratic Party said.']\n",
      "['She isn’t optimistic that Congress will help schools reopen safely in the fall: “There’s going to be a lot of parents in tears.”']\n",
      "['Bobby White was celebrated as the “Basketball Cop” after millions saw a video of him shooting hoops with local teenagers.', 'A recording from two years earlier shows him throwing a young Black man on the hood of his car during an arrest.']\n",
      "['On a virtual fund-raiser, Mr. Biden also mused aloud about filling positions like secretary of state, attorney general and chief of staff, but said, “I haven’t asked anybody yet.’’']\n",
      "['The president went to Phoenix to speak to a group of student supporters.']\n",
      "['A group of worst-case scenario planners — mostly Democrats, but also some anti-Trump Republicans — have been gaming out how to respond to various doomsday options for the 2020 presidential election.']\n",
      "['Two of our correspondents break down highlights from the Democrats’ first virtual convention.']\n",
      "['Black women have long known this.', 'With Kamala Harris as Biden’s V.P. pick, perhaps there’s an opportunity to change the idea that aspirations are something to apologize for.']\n",
      "['“Everybody just assumes no one is going,” said one House member wary of the virus risks.', 'But other delegates dismissed the health threat and said it was an honor to help nominate President Trump.']\n",
      "['An abortion rights ruling leads to a new focus for Senate races: This is your morning tip sheet.']\n",
      "['Joe Biden may have a lead in most polls, but it’s not anywhere close to a comfortable one.']\n",
      "['Anti-abortion groups hope to keep Americans voting Republican despite anger at leaders’ handling of the coronavirus, race and the economy.', 'Abortion-rights groups say the issues are all linked.']\n",
      "['For Democrats urging Joe Biden to run alongside a black woman, the choice is far more than symbolic.']\n",
      "['As Biden works to win over progressive voters, he has empowered a Washington uber-veteran long trailed by allegations of personal and financial indiscretion.']\n",
      "['Bernie Sanders says winning a plurality of delegates is good enough for the nomination.', 'His rivals say a majority is needed.', 'What does that mean?', 'And why are superdelegates coming up again?']\n",
      "['A story about protesters burning Bibles drew condemnation from conservatives.', 'It now appears to have been wildly exaggerated — and the first viral hit in Russia’s 2020 disinformation campaign.']\n",
      "['The letter came in response to a November request from two Republican senators seeking Obama administration meeting records related to Ukrainian officials.']\n",
      "['Will the thousands of Trump supporters wear masks?', 'What time is the rally?', 'How will President Trump talk about race?', 'What protests will he face?', 'Here’s what you need to know.']\n",
      "['Republican incumbents in swing states have struggled to disentangle themselves from President Trump, whose voters are loyal but represent a minority of the electorate.']\n",
      "['The Trump campaign is trying to make sure that Mr. Trump’s message will be almost impossible to miss even during the Democrats’ biggest week.']\n",
      "['Former President Barack Obama endorsed 118 candidates, hoping to tip the scales in key battlegrounds.', 'Primary races in several states on Tuesday have the attention of national party leaders.']\n",
      "['On the brink of actual voting, the party that preached of unity to defeat President Trump finds itself very divided.']\n",
      "['Ms. Klobuchar made her decision hours before Super Tuesday.', 'She shocked the primary field with a third-place finish in New Hampshire, but ultimately could not compete with better-funded rivals.']\n",
      "['A day before the caucuses, the state party says volunteer and staff members at caucus sites are forbidden to talk to the news media.']\n",
      "['Gov. Gavin Newsom on Friday made California the first state to alter its plans for the general election in response to the coronavirus pandemic.']\n",
      "['Bernie Sanders’s campaign said it would request the more intensive step of a recount after the state party announced that it had changed results in 29 precincts but that no national delegates would be moved.']\n",
      "['Here’s a sampling of the reactions to the first one-on-one debate between the two remaining major Democratic candidates.']\n",
      "['Which states do campaigns view as true battlegrounds?', 'Follow the money.']\n",
      "['Republicans increasingly believe that elevating China’s culpability for spreading the coronavirus may be the best way to improve their difficult election chances.', 'The president is muddying the message.']\n",
      "['Daily briefings during the crisis in New York have given Gov. Andrew Cuomo a captive audience, and a growing fan base.', 'But he has made it clear that he’s not going anywhere.']\n",
      "['No Democrat has won or broken even with voters over 65 in two decades.', 'But seniors’ dismay about President Trump could change that.']\n",
      "['President Trump, in a tweet Tuesday night, expressed his full-throated endorsement of the man running against his former attorney general in a runoff election for the state’s primary.']\n",
      "['Former campaign workers for Michael R. Bloomberg’s presidential bid reacted angrily on Monday to news that they would not work through the November election, as expected.']\n",
      "['Both President Trump and Joseph R. Biden Jr.', 'have called for justice for Mr. Floyd.', 'Mr. Trump has also called protesters “thugs,” and Mr. Biden has blamed him for “incendiary tweets.”']\n",
      "['The Times asked the Democratic presidential candidates for their views on housing and homelessness.', '(Joseph R. Biden Jr.', 'and Tulsi Gabbard did not complete the survey.)']\n",
      "['Campaign veterans reacted to how the Democratic presidential candidates fared Tuesday night.']\n",
      "['Mr. Biden’s attack on his rival in Iowa came hours after another leading candidate, Pete Buttigieg, criticized both men, in a day of sparring that underscored the pressure they all face to excel in Monday’s caucuses.']\n",
      "['On The Federalist and in Fox News appearances, some of President Trump’s supporters don’t really defend him.', 'They attack the critics.']\n",
      "['With three of his top rivals pinned down at the Senate impeachment trial, Mr. Buttigieg has seized the opportunity to camp out in the state, largely below the radar.']\n",
      "['Protests planned for Friday continue a decade-long partisan cleaving in the state and serve as a stand-in for the general election battle to come.']\n",
      "['It was the strongest sign yet that he plans to keep competing against Joe Biden in the Democratic presidential primary.']\n",
      "['Mr. Steyer, a billionaire running for president, rents his campaign headquarters in South Carolina from a daughter of the state’s most powerful Democrat, Representative James Clyburn.']\n",
      "['Mr. Sanders’s total is the largest three-month haul disclosed by any candidate so far in the Democratic primary race.']\n",
      "['The Trump campaign is spending millions on ads that promote a dark and exaggerated portrayal of Democratic-led cities, a tactic that reinforces his “law and order’’ campaign message.']\n",
      "['Technical glitches disrupted Mr. Biden’s virtual town hall with voters in Illinois on Friday.', 'Bernie Sanders held an online “fireside chat’’ on Saturday.', 'It’s the new paradigm for candidates amid a health crisis.']\n",
      "['Paying attention to the impeachment trial this week?', 'Us too.', 'But here are 7 things that happened on the campaign trail you may have missed.']\n",
      "['The president’s campaign manager and his allies commandeered Republican voter data and fund-raising engines, consolidating power — and profiting — in ways never before possible.']\n"
     ]
    }
   ],
   "source": [
    "for i, el in enumerate(data[\"Summary\"]):\n",
    "    print(punkt_tokenizer.tokenize(text=el))\n",
    "    if i == 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vado a calcolare il numero di frasi che vengono tokenizzate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Test su 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T12:04:58.190976Z",
     "start_time": "2021-02-11T12:04:58.097322Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "parser = PlaintextParser.from_string(data[\"Paragraphs_as_string\"][1], \n",
    "                                     tokenizer = Tokenizer('english'))\n",
    "\n",
    "stemmer = Stemmer('english')\n",
    "summarizer = LsaSummarizer(stemmer)\n",
    "\n",
    "summarizer.stop_words = get_stop_words('english')\n",
    "LSA_summaries = [sentence for sentence in summarizer(parser.document, 2)]\n",
    "\n",
    "reference_sentences = [\n",
    "    Sentence(el, tokenizer=Tokenizer(\"english\"))\n",
    "    for el in sent_tokenize(data[\"Summary\"][1])\n",
    "]\n",
    "\n",
    "print(\"Rouge_1\", rouge_1(evaluated_sentences=LSA_summaries,\n",
    "        reference_sentences=reference_sentences))\n",
    "print(\"Rouge_2\", rouge_2(evaluated_sentences=LSA_summaries,\n",
    "        reference_sentences=reference_sentences))\n",
    "\n",
    "rouge_l_sentence_level(evaluated_sentences=LSA_summaries,\n",
    "        reference_sentences=reference_sentences)\n",
    "\n",
    "rouge_l_summary_level(evaluated_sentences=LSA_summaries,\n",
    "        reference_sentences=reference_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script su tutti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T13:58:46.850725Z",
     "start_time": "2021-02-11T13:58:46.844154Z"
    }
   },
   "outputs": [],
   "source": [
    "data = data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T14:28:02.163160Z",
     "start_time": "2021-02-11T13:58:46.853038Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "N/A% (0 of 9480) |                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " LSA \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (9480 of 9480) |####################| Elapsed Time: 0:07:50 Time:  0:07:50\n",
      "  0% (3 of 9480) |                       | Elapsed Time: 0:00:00 ETA:   0:05:28"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Lunh \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (9480 of 9480) |####################| Elapsed Time: 0:05:12 Time:  0:05:12\n",
      "  0% (7 of 9480) |                       | Elapsed Time: 0:00:00 ETA:   0:03:51"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SumBasic \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (9480 of 9480) |####################| Elapsed Time: 0:02:20 Time:  0:02:20\n",
      "  0% (3 of 9480) |                       | Elapsed Time: 0:00:00 ETA:   0:06:03"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " LexRank \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (9480 of 9480) |####################| Elapsed Time: 0:07:12 Time:  0:07:12\n",
      "  0% (3 of 9480) |                       | Elapsed Time: 0:00:00 ETA:   0:06:02"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " KLS \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (9480 of 9480) |####################| Elapsed Time: 0:06:37 Time:  0:06:37\n"
     ]
    }
   ],
   "source": [
    "stemmer = Stemmer('english')\n",
    "summarizer_LSA = LsaSummarizer(stemmer)\n",
    "summarizer_Luhn = LuhnSummarizer(stemmer)\n",
    "summarizer_Sum = SumBasicSummarizer(stemmer)\n",
    "summarizer_Lex = LexRankSummarizer(stemmer)\n",
    "# summarizer_KLS = KLSummarizer(stemmer)\n",
    "summarizer_Random = RandomSummarizer(stemmer)\n",
    "summarizer_Red = ReductionSummarizer(stemmer)\n",
    "for summarizer in [\n",
    "        summarizer_LSA, summarizer_Luhn, summarizer_Sum, \n",
    "    summarizer_Lex, summarizer_Red\n",
    "]:\n",
    "    summarizer.stop_words = get_stop_words('english')\n",
    "\n",
    "dict_res = {}\n",
    "for name, summarizer in zip(\n",
    "    [\"LSA\", \"Lunh\", \"SumBasic\", \"LexRank\", \"Reduction\"], [\n",
    "        summarizer_LSA, summarizer_Luhn, summarizer_Sum, summarizer_Lex,\n",
    "        summarizer_Red\n",
    "    ]):\n",
    "    print(\"\\n\", name, \"\\n\")\n",
    "    results_rouge_1 = []\n",
    "    results_rouge_2 = []\n",
    "    results_rouge_l_1 = []\n",
    "    results_rouge_l_2 = []\n",
    "    for i in progressbar.progressbar(range(len(data))):\n",
    "        (article, summary) = (data[\"Paragraphs_as_string\"][i],\n",
    "                              data[\"Summary\"][i])\n",
    "        try:\n",
    "            parser = PlaintextParser.from_string(\n",
    "                article, tokenizer=Tokenizer('english'))\n",
    "\n",
    "            summaries = [\n",
    "                sentence for sentence in summarizer(parser.document, 2)\n",
    "            ]\n",
    "\n",
    "            #     To use sumy's evaluation functions, I need to have the text in\n",
    "            #     Sentence objects\n",
    "            reference_sentences = [\n",
    "                Sentence(sent, tokenizer=Tokenizer(\"english\"))\n",
    "                for sent in sent_tokenize(summary)\n",
    "            ]\n",
    "            results_rouge_1.append(\n",
    "                rouge_1(evaluated_sentences=summaries,\n",
    "                        reference_sentences=reference_sentences))\n",
    "\n",
    "            results_rouge_2.append(\n",
    "                rouge_2(evaluated_sentences=summaries,\n",
    "                        reference_sentences=reference_sentences))\n",
    "\n",
    "            results_rouge_l_1.append(\n",
    "                rouge_l_sentence_level(\n",
    "                    evaluated_sentences=summaries,\n",
    "                    reference_sentences=reference_sentences))\n",
    "\n",
    "            results_rouge_l_2.append(\n",
    "                rouge_l_summary_level(evaluated_sentences=summaries,\n",
    "                                      reference_sentences=reference_sentences))\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "#         Save results and progress to next summarizer\n",
    "    dict_res[name] = {\n",
    "        \"Rouge_1\": results_rouge_1,\n",
    "        \"Rouge_2\": results_rouge_2,\n",
    "        \"Rouge_L_sentence_level\": results_rouge_l_1,\n",
    "        \"Rouge_L_summary_level\": results_rouge_l_2\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T14:44:00.641277Z",
     "start_time": "2021-02-11T14:44:00.606545Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create pandas dataframe for mean of results\n",
    "res_mean = pd.DataFrame(columns = dict_res.keys())\n",
    "# Dataframe for std of results\n",
    "res_se = pd.DataFrame(columns = dict_res.keys())\n",
    "for col in res_mean:\n",
    "    res_mean[col] = pd.Series(\n",
    "        {key: np.mean(value)\n",
    "         for key, value in dict_res[col].items()})\n",
    "    res_se[col] = pd.Series(\n",
    "        {key: np.std(value)/np.sqrt(len(value))\n",
    "         for key, value in dict_res[col].items()})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T14:44:02.081456Z",
     "start_time": "2021-02-11T14:44:02.071553Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LSA</th>\n",
       "      <th>Lunh</th>\n",
       "      <th>SumBasic</th>\n",
       "      <th>LexRank</th>\n",
       "      <th>Reduction</th>\n",
       "      <th>Random</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Rouge_1</th>\n",
       "      <td>0.393832</td>\n",
       "      <td>0.418053</td>\n",
       "      <td>0.230668</td>\n",
       "      <td>0.374518</td>\n",
       "      <td>0.448022</td>\n",
       "      <td>0.245043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rouge_2</th>\n",
       "      <td>0.167964</td>\n",
       "      <td>0.219488</td>\n",
       "      <td>0.076637</td>\n",
       "      <td>0.191279</td>\n",
       "      <td>0.242454</td>\n",
       "      <td>0.070406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rouge_L_sentence_level</th>\n",
       "      <td>0.138476</td>\n",
       "      <td>0.180001</td>\n",
       "      <td>0.147127</td>\n",
       "      <td>0.185174</td>\n",
       "      <td>0.179566</td>\n",
       "      <td>0.124861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rouge_L_summary_level</th>\n",
       "      <td>0.014459</td>\n",
       "      <td>0.018343</td>\n",
       "      <td>0.037052</td>\n",
       "      <td>0.024488</td>\n",
       "      <td>0.015887</td>\n",
       "      <td>0.027457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             LSA      Lunh  SumBasic   LexRank  Reduction  \\\n",
       "Rouge_1                 0.393832  0.418053  0.230668  0.374518   0.448022   \n",
       "Rouge_2                 0.167964  0.219488  0.076637  0.191279   0.242454   \n",
       "Rouge_L_sentence_level  0.138476  0.180001  0.147127  0.185174   0.179566   \n",
       "Rouge_L_summary_level   0.014459  0.018343  0.037052  0.024488   0.015887   \n",
       "\n",
       "                          Random  \n",
       "Rouge_1                 0.245043  \n",
       "Rouge_2                 0.070406  \n",
       "Rouge_L_sentence_level  0.124861  \n",
       "Rouge_L_summary_level   0.027457  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:04:33.261193Z",
     "start_time": "2021-02-11T17:04:33.251964Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LSA</th>\n",
       "      <th>Lunh</th>\n",
       "      <th>SumBasic</th>\n",
       "      <th>LexRank</th>\n",
       "      <th>Reduction</th>\n",
       "      <th>Random</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Rouge_1</th>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.003010</td>\n",
       "      <td>0.002125</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.003045</td>\n",
       "      <td>0.002042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rouge_2</th>\n",
       "      <td>0.003131</td>\n",
       "      <td>0.003601</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.003447</td>\n",
       "      <td>0.003677</td>\n",
       "      <td>0.002053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rouge_L_sentence_level</th>\n",
       "      <td>0.001659</td>\n",
       "      <td>0.002034</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.002064</td>\n",
       "      <td>0.001988</td>\n",
       "      <td>0.001387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rouge_L_summary_level</th>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             LSA      Lunh  SumBasic   LexRank  Reduction  \\\n",
       "Rouge_1                 0.002632  0.003010  0.002125  0.003000   0.003045   \n",
       "Rouge_2                 0.003131  0.003601  0.002137  0.003447   0.003677   \n",
       "Rouge_L_sentence_level  0.001659  0.002034  0.001488  0.002064   0.001988   \n",
       "Rouge_L_summary_level   0.000073  0.000104  0.000211  0.000159   0.000095   \n",
       "\n",
       "                          Random  \n",
       "Rouge_1                 0.002042  \n",
       "Rouge_2                 0.002053  \n",
       "Rouge_L_sentence_level  0.001387  \n",
       "Rouge_L_summary_level   0.000169  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:13:31.820940Z",
     "start_time": "2021-02-11T17:13:31.812338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving evaluation averages\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] Saving evaluation averages\")\n",
    "with open(config_var['output_folder']+\"/avgs.csv\", 'w') as file:\n",
    "    res_mean.to_csv(file)\n",
    "with open(config_var['output_folder']+\"/ses.csv\", 'w') as file:\n",
    "    res_se.to_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T13:56:24.098562Z",
     "start_time": "2021-02-12T13:56:23.616570Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading configuration\n",
      "[INFO] Loading Rouge results\n",
      "[INFO] Loading json data from scraping_data_NYTimes.json\n",
      "[INFO] Removing articles without summary or paragraphs\n",
      "[INFO] Size before cleaning: 10335\n",
      "[INFO] Size after cleaning: 9480\n"
     ]
    }
   ],
   "source": [
    "print('[INFO] Loading configuration')\n",
    "with open(\"./config.yml\", 'r') as file:\n",
    "    config_var = safe_load(file)[\"main\"]\n",
    "\n",
    "print(\"[INFO] Loading Rouge results\")\n",
    "with open(config_var[\"output_folder\"]+\"/avgs.csv\", 'r') as file:\n",
    "    avgs = pd.read_csv(file, index_col = 0)\n",
    "with open(config_var[\"output_folder\"]+\"/ses.csv\", 'r') as file:\n",
    "    ses = pd.read_csv(file, index_col = 0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T09:28:05.538767Z",
     "start_time": "2021-02-12T09:28:05.535585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge_1\n",
      "Rouge_2\n",
      "Rouge_L_sentence_level\n",
      "Rouge_L_summary_level\n"
     ]
    }
   ],
   "source": [
    "for el in avgs.index:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T16:19:17.466985Z",
     "start_time": "2021-02-12T16:19:17.460680Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_clean_data(path_to_file=None):\n",
    "    print(\"[INFO] Loading json data from\", path_to_file)\n",
    "    with open(path_to_file, 'r') as file:\n",
    "        data = pd.DataFrame(json.load(file))\n",
    "\n",
    "    print(\"[INFO] Removing articles without summary or paragraphs\")\n",
    "    print(\"[INFO] Size before cleaning:\", len(data))\n",
    "    data = data[(data[\"Summary\"].map(len) >= 1)\n",
    "                & (data[\"Paragraphs\"].map(len) >= 1)]\n",
    "    print(\"[INFO] Size after cleaning:\", len(data))\n",
    "\n",
    "    print(\"[INFO] Removing summaries where the first word is 'By'\")\n",
    "    print(\"[INFO] Size before cleaning:\", len(data))\n",
    "    data = (data[data[\"Summary\"].apply(lambda x: (x[0:2] != \"By\"))])\n",
    "    print(\"[INFO] Size after cleaning:\", len(data))\n",
    "    print(\"[INFO] Remove 'daily briefing' articles.\")\n",
    "    week_day = [\n",
    "        \"Monday:\", \"Tuesday:\", \"Wednesday:\", 'Thursday:', \"Friday:\",\n",
    "        \"Saturday:\", \"Sunday:\"\n",
    "    ]\n",
    "    print(\"[INFO] Size before cleaning:\", len(data))\n",
    "    data = (data[data[\"Summary\"].apply(lambda x:\n",
    "                                       (x.split(\" \")[0] not in week_day))])\n",
    "    print(\"[INFO] Size after cleaning:\", len(data))\n",
    "    return data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T16:19:18.899344Z",
     "start_time": "2021-02-12T16:19:18.250597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\t SUMMARIZATION MANUAL EVALUATION\t\t\n",
      "\n",
      "[INFO] Loading configuration.\n",
      "[INFO] Loading generated summaries.\n",
      "[INFO] Loading json data from ./datasets/scraping_data_NYTimes.json\n",
      "[INFO] Removing articles without summary or paragraphs\n",
      "[INFO] Size before cleaning: 10335\n",
      "[INFO] Size after cleaning: 9480\n",
      "[INFO] Removing summaries where the first word is 'By'\n",
      "[INFO] Size before cleaning: 9480\n",
      "[INFO] Size after cleaning: 9065\n",
      "[INFO] Remove 'daily briefing' articles.\n",
      "[INFO] Size before cleaning: 9065\n",
      "[INFO] Size after cleaning: 8972\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\t\\t SUMMARIZATION MANUAL EVALUATION\\t\\t\\n\")\n",
    "print('[INFO] Loading configuration.')\n",
    "with open(\"./config.yml\", 'r') as file:\n",
    "    config_var = safe_load(file)[\"main\"]\n",
    "print(\"[INFO] Loading generated summaries.\")\n",
    "with open(config_var[\"output_folder\"] + \"/summaries.json\", 'r') as file:\n",
    "    summaries = pd.DataFrame(json.load(file))\n",
    "\n",
    "data = load_clean_data(path_to_file=str(config_var['dataset_folder']) + \"/\" +\n",
    "                       str(config_var['data_to_use']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T16:19:23.393957Z",
     "start_time": "2021-02-12T16:19:23.391444Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T08:59:18.288385Z",
     "start_time": "2021-02-13T08:59:18.282315Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-2f4ba223d1d4>, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-2f4ba223d1d4>\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    with open(\"\")\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "##### Initiate empty dictionary\n",
    "score_dict = {}\n",
    "for col in summaries:\n",
    "    score_dict[col] = {}\n",
    "print(\"[INFO] Evaluation Time\")\n",
    "for n, idx in enumerate(\n",
    "        random.sample(range(0, len(summaries)), config_var[\"num_eval\"])):\n",
    "    print(\"[INFO] Test number\", n)\n",
    "    print('[INFO] Real summary:\\n\\n«', data[\"Summary\"][idx], \"»\\n\")\n",
    "    for col in summaries:\n",
    "        print(\"[INFO] Generate summary by \" + str(col) + \":\\n\\n«\",\n",
    "              summaries[col][idx], \"»\\n\")\n",
    "        while True:\n",
    "            print(\"[INPUT] Give score [1-5]:\")\n",
    "            score = int(input())\n",
    "            if score > 5 or score < 1:\n",
    "                print(\"[ERROR] Please give number between 1 and 5 (included).\")\n",
    "                pass\n",
    "            else:\n",
    "                break\n",
    "        score_dict[col].update({idx: score})\n",
    "print(\"[INFO] Saving manual evaluations.\")\n",
    "\n",
    "with open(config_var['output_folder']+\"/manual_evaluation.json\", 'w') as file:\n",
    "    json.dump(score_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
